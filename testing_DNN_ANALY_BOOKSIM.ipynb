{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import regularizers\n",
    "import random\n",
    "import subprocess as sp\n",
    "import os\n",
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = load_model('saved_models/best_model2.h5') \n",
    "train_stats = pd.read_pickle('saved_models/mode2_stat.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    return (x - train_stats['mean']) / (train_stats['std']+0.0000001)\n",
    "\n",
    "def log_norm(x):\n",
    "    for col in x.columns:\n",
    "        x.iloc[:,col] = (x.iloc[:,col] - x.iloc[:,col].min()+1).transform(np.log10)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3541ac683f16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtrain_MSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train MSE = %f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_MSE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "# verifying the model with trained data\n",
    "X_train = pd.read_csv('dataset/updated_model_dataset/Final_dataset/vopd_dataset.csv',header=None,error_bad_lines=False)\n",
    "Y_train = X_train.iloc[:,-1]\n",
    "X_train.drop(X_train.columns[-1],inplace=True,axis=1)\n",
    "# X_train.drop(X_train.columns[-1],inplace=True,axis=1)\n",
    "X = norm(X_train)\n",
    "X.drop(X.columns[0],inplace=True,axis=1)\n",
    "Y_pred = model.predict(X).flatten()\n",
    "train_MSE = np.sum(np.square(Y_pred-Y_train))/Y_train.shape[0]\n",
    "print('train MSE = %f'%(train_MSE))\n",
    "\n",
    "#plot histogram\n",
    "train_error = Y_pred-Y_train\n",
    "plt.hist(train_error, bins =25,alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel(\"Prediction Error\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title('train_data_hist')\n",
    "plt.savefig('train_histr.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapp_syntraff(traffic_type,inject_rate):\n",
    "#     random.seed(25)\n",
    "         \n",
    "    file2 = open(\"latency_model_2010_pso/files/vopd_mapp.txt\",\"w\")\n",
    "    file1 = open('booksim/examples/vopd_mapp.txt','w')\n",
    "    for i in range(1000):\n",
    "        mapp_vec = random.sample(range(1,17),16)\n",
    "        file2.write('%d\\t %s \\t %f\\n'%(i,str(mapp_vec),inject_rate))\n",
    "        file1.write('%d\\t %s \\t %f\\n'%(i,str(mapp_vec),inject_rate))\n",
    "    file2.close()\n",
    "    file1.close()\n",
    "    \n",
    "    # ************* begin modify file  files/config.txt for diff syn traffic pattern *************#\n",
    "    with open('latency_model_2010_pso/files/config.txt', 'r') as file:\n",
    "        # read a list of lines into data\n",
    "        data = file.readlines()\n",
    "\n",
    "\n",
    "    # now change the 2nd line, note that you have to add a newline\n",
    "    data[3] = 'traffic file#\\tfiles/'+traffic_type+'\\n'\n",
    "\n",
    "    # and write everything back\n",
    "    with open('latency_model_2010_pso/files/config.txt', 'w') as file:\n",
    "        file.writelines( data )\n",
    "    #************** end modifying the file **************************************************#\n",
    "    open('latency_model_2010_pso/files/vopd_mapp_features.txt','w').close()\n",
    "    os.chdir('/home/eslab_server/Desktop/Ramesh/queuing_work/mesh4x4_training/latency_model_2010_pso/')\n",
    "    os.system('./latency_model_2010_pso')\n",
    "    os.chdir('/home/eslab_server/Desktop/Ramesh/queuing_work/mesh4x4_training/')\n",
    "\n",
    "    with open('latency_model_2010_pso/files/vopd_mapp_features.txt','r') as my_file:\n",
    "        text = my_file.read()\n",
    "        text = text.replace(\"    \", \" \")\n",
    "        text = text.replace(\"[\", \" \")\n",
    "        text = text.replace(\"]\", \" \")\n",
    "        text = text.replace(\" \",\",\")\n",
    "        text = text.replace(\",,,\",\",\")\n",
    "        text = text.replace(\",,\",\",\")\n",
    "        text = text.replace(\",,\",\",\")\n",
    "        text = text.replace(\",\\n,\\n,\\n,\",'\\n')\n",
    "        text = text.replace('\\n,',',')\n",
    "    my_file.close()\n",
    "    \n",
    "    open('latency_model_2010_pso/files/vopd_mapp_features.csv', 'w').close()\n",
    "    with open('latency_model_2010_pso/files/vopd_mapp_features.csv','w') as analy_file:\n",
    "        analy_file.write(text)\n",
    "\n",
    "\n",
    "    # Latency evaluation using booksim simulator\n",
    "    os.chdir('/home/eslab_server/Desktop/Ramesh/queuing_work/mesh4x4_training/booksim/')\n",
    "#     print(os.getcwd())\n",
    "        # ************* begin modify file  files/config.txt for diff syn traffic pattern *************#\n",
    "    with open('examples/anynet_config', 'r') as file:\n",
    "        # read a list of lines into data\n",
    "        trff = file.readlines()\n",
    "\n",
    "\n",
    "    # now change the 2nd line, note that you have to add a newline\n",
    "    trff[73] = 'traffic_file = examples/'+traffic_type+';\\n'\n",
    "\n",
    "    # and write everything back\n",
    "    with open('examples/anynet_config', 'w') as file:\n",
    "        file.writelines(trff)\n",
    "    #************** end modifying the file **************************************************#\n",
    "    \n",
    "    open('examples/vopd_mapp_out.txt', 'w').close()\n",
    "    sp.call('./pgm_within_pgm',shell=True)\n",
    "\n",
    "    with open('examples/vopd_mapp_out.txt','r') as my_file:\n",
    "        text = my_file.read()\n",
    "        text = text.replace(\" \\t \", \",\")\n",
    "        text = text.replace(\"\\t\\t\", \",\")\n",
    "        text = text.replace(\"\\t\", \",\")\n",
    "        text = text.replace(\"[\", \"\")\n",
    "        text = text.replace(\"]\", \" \")\n",
    "        text = text.replace(\" \",\"\")\n",
    "    my_file.close()\n",
    "    open('/home/eslab_server/Desktop/Ramesh/queuing_work/mesh4x4_training/booksim_out.csv', 'w').close()\n",
    "    with open('/home/eslab_server/Desktop/Ramesh/queuing_work/mesh4x4_training/booksim_out.csv','w') as myfile:\n",
    "        myfile.write(text)\n",
    "    my_file.close()\n",
    "#     print(os.getcwd())\n",
    "    os.chdir('/home/eslab_server/Desktop/Ramesh/queuing_work/mesh4x4_training')\n",
    "#     print(os.getcwd())\n",
    "\n",
    "    X = pd.read_csv(\"latency_model_2010_pso/files/vopd_mapp_features.csv\",header=None,\n",
    "                          error_bad_lines=False)\n",
    "    book_lat = pd.read_csv('booksim_out.csv',header=None,\n",
    "                          error_bad_lines=False)\n",
    "    X.drop(X.columns[-1],inplace=True,axis=1)\n",
    "    anay_lat = X.iloc[:,-1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    X_norm = norm(X)\n",
    "    X_norm.drop(X_norm.columns[0],inplace=True,axis=1)\n",
    "    Y = model.predict(X_norm).flatten()\n",
    "\n",
    "    return anay_lat,Y,book_lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/eslab_server/Desktop/Ramesh/queuing_work/mesh4x4_training/')\n",
    "\n",
    "# trff_files = ['traffic_bitcomplement_16.txt','traffic_bitreverse_16.txt',\n",
    "#               'traffic_neighbor_16.txt','traffic_shuffle_16.txt','traffic_tornado_16.txt',\n",
    "#               'traffic_transpose_16.txt','traffic_uniform_16.txt','traffic_vopd.txt','traffic_vopd.txt']\n",
    "\n",
    "trff_files = ['traffic_vopd.txt','traffic_vopd.txt','traffic_vopd.txt','traffic_vopd.txt',\n",
    "              'traffic_vopd.txt','traffic_vopd.txt','traffic_vopd.txt','traffic_vopd.txt',\n",
    "              'traffic_vopd.txt']\n",
    "\n",
    "anay_lat,Y,book_lat = mapp_syntraff(trff_files[0],0.007)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error b/w booksim and analy model=0.038216\n"
     ]
    }
   ],
   "source": [
    "#  Relative error between analy model and Booksim\n",
    "err_anal = np.abs(book_lat.iloc[:,18]-anay_lat)\n",
    "rel_err = np.sum(err_anal/book_lat.iloc[:,18])/len(err_anal)\n",
    "print('Relative error b/w booksim and analy model=%f'%rel_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error b/w booksim and DNN model=0.029736\n"
     ]
    }
   ],
   "source": [
    "#  Relative error between analy model and Booksim\n",
    "err_DNN = np.abs(book_lat.iloc[:,18]-Y)\n",
    "rel_err = np.sum(err_DNN/book_lat.iloc[:,18])/len(err_DNN)\n",
    "print('Relative error b/w booksim and DNN model=%f'%rel_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error b/w booksim and analy model= 0.1441213550301619\n"
     ]
    }
   ],
   "source": [
    " book_lat = pd.read_csv('booksim_out.csv',header=None,error_bad_lines=False)\n",
    "err_anal = np.zeros(1000)\n",
    "rel_err = np.zeros(1000)\n",
    "exp = [677]\n",
    "for i in range(len(anay_lat)):\n",
    "    if i not in exp:\n",
    "        err_anal[i] = np.abs(book_lat.iloc[:,18][i]-anay_lat[i])\n",
    "        rel_err[i]  = err_anal[i]/book_lat.iloc[:,18][i]\n",
    "print('Relative error b/w booksim and analy model=',np.sum(rel_err)/len(rel_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
